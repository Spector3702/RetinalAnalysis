{"cells":[{"cell_type":"markdown","metadata":{},"source":["**Import Tensorflow & Varify GPU**\n","- [MacOS](https://medium.com/geekculture/installing-tensorflow-on-apple-silicon-84a28050d784)"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Found GPU at: /device:GPU:0\n"]},{"name":"stderr","output_type":"stream","text":["2023-12-28 23:27:11.904630: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n","2023-12-28 23:27:11.904649: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n","2023-12-28 23:27:11.904654: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n","2023-12-28 23:27:11.904713: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n","2023-12-28 23:27:11.904747: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"]}],"source":["import tensorflow as tf\n","\n","device_name = tf.test.gpu_device_name()\n","if not device_name:\n","    print(\"GPU device not found\")\n","else:\n","    print(f\"Found GPU at: {device_name}\")"]},{"cell_type":"markdown","metadata":{"id":"Pp_x_psEty8M"},"source":["**Packages and data preprocessing**"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import os \n","import cv2\n","import random\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np\n","\n","from tqdm import tqdm\n","from glob import glob\n","\n","from warnings import filterwarnings\n","filterwarnings('ignore')"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"p-BOet9PrCgd","outputId":"258c7072-5f27-476e-f7b8-2bc71cdf88c7"},"outputs":[],"source":["X_train = pd.read_csv('data/Training_Set/RFMiD_Training_Labels.csv')\n","X_val = pd.read_csv('data/Evaluation_Set/RFMiD_Validation_Labels.csv')\n","X_test = pd.read_csv('data/Test_Set/RFMiD_Testing_Labels.csv')"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"KOWOE7D7pGJY","outputId":"9e88622d-5bba-4a98-c349-65d713e3571e"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ID</th>\n","      <th>Disease_Risk</th>\n","      <th>DR</th>\n","      <th>ARMD</th>\n","      <th>MH</th>\n","      <th>DN</th>\n","      <th>MYA</th>\n","      <th>BRVO</th>\n","      <th>TSLN</th>\n","      <th>ERM</th>\n","      <th>...</th>\n","      <th>CME</th>\n","      <th>PTCR</th>\n","      <th>CF</th>\n","      <th>VH</th>\n","      <th>MCA</th>\n","      <th>VS</th>\n","      <th>BRAO</th>\n","      <th>PLQ</th>\n","      <th>HPED</th>\n","      <th>CL</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 47 columns</p>\n","</div>"],"text/plain":["   ID  Disease_Risk  DR  ARMD  MH  DN  MYA  BRVO  TSLN  ERM  ...  CME  PTCR  \\\n","0   1             1   1     0   0   0    0     0     0    0  ...    0     0   \n","1   2             1   1     0   0   0    0     0     0    0  ...    0     0   \n","2   3             1   1     0   0   0    0     0     0    0  ...    0     0   \n","3   4             1   0     0   1   0    0     0     0    0  ...    0     0   \n","4   5             1   1     0   0   0    0     0     0    0  ...    0     0   \n","\n","   CF  VH  MCA  VS  BRAO  PLQ  HPED  CL  \n","0   0   0    0   0     0    0     0   0  \n","1   0   0    0   0     0    0     0   0  \n","2   0   0    0   0     0    0     0   0  \n","3   0   0    0   0     0    0     0   0  \n","4   0   0    0   0     0    0     0   0  \n","\n","[5 rows x 47 columns]"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["X_train.head()"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"Pu9zLg54qHPH","outputId":"4369b563-439f-4f88-ca4e-907c7e90324d"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ID</th>\n","      <th>Disease_Risk</th>\n","      <th>DR</th>\n","      <th>ARMD</th>\n","      <th>MH</th>\n","      <th>DN</th>\n","      <th>MYA</th>\n","      <th>BRVO</th>\n","      <th>TSLN</th>\n","      <th>ERM</th>\n","      <th>...</th>\n","      <th>CME</th>\n","      <th>PTCR</th>\n","      <th>CF</th>\n","      <th>VH</th>\n","      <th>MCA</th>\n","      <th>VS</th>\n","      <th>BRAO</th>\n","      <th>PLQ</th>\n","      <th>HPED</th>\n","      <th>CL</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>1920.000000</td>\n","      <td>1920.000000</td>\n","      <td>1920.000000</td>\n","      <td>1920.000000</td>\n","      <td>1920.000000</td>\n","      <td>1920.000000</td>\n","      <td>1920.000000</td>\n","      <td>1920.000000</td>\n","      <td>1920.000000</td>\n","      <td>1920.000000</td>\n","      <td>...</td>\n","      <td>1920.000000</td>\n","      <td>1920.000000</td>\n","      <td>1920.000000</td>\n","      <td>1920.000000</td>\n","      <td>1920.000000</td>\n","      <td>1920.000000</td>\n","      <td>1920.000000</td>\n","      <td>1920.000000</td>\n","      <td>1920.000000</td>\n","      <td>1920.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>960.500000</td>\n","      <td>0.791146</td>\n","      <td>0.195833</td>\n","      <td>0.052083</td>\n","      <td>0.165104</td>\n","      <td>0.071875</td>\n","      <td>0.052604</td>\n","      <td>0.038021</td>\n","      <td>0.096875</td>\n","      <td>0.007292</td>\n","      <td>...</td>\n","      <td>0.002083</td>\n","      <td>0.002604</td>\n","      <td>0.001563</td>\n","      <td>0.000521</td>\n","      <td>0.000521</td>\n","      <td>0.000521</td>\n","      <td>0.001042</td>\n","      <td>0.000521</td>\n","      <td>0.000521</td>\n","      <td>0.000521</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>554.400577</td>\n","      <td>0.406596</td>\n","      <td>0.396944</td>\n","      <td>0.222253</td>\n","      <td>0.371371</td>\n","      <td>0.258348</td>\n","      <td>0.223300</td>\n","      <td>0.191296</td>\n","      <td>0.295865</td>\n","      <td>0.085102</td>\n","      <td>...</td>\n","      <td>0.045608</td>\n","      <td>0.050978</td>\n","      <td>0.039508</td>\n","      <td>0.022822</td>\n","      <td>0.022822</td>\n","      <td>0.022822</td>\n","      <td>0.032266</td>\n","      <td>0.022822</td>\n","      <td>0.022822</td>\n","      <td>0.022822</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>480.750000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>960.500000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>1440.250000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>1920.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>...</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>8 rows × 47 columns</p>\n","</div>"],"text/plain":["                ID  Disease_Risk           DR         ARMD           MH  \\\n","count  1920.000000   1920.000000  1920.000000  1920.000000  1920.000000   \n","mean    960.500000      0.791146     0.195833     0.052083     0.165104   \n","std     554.400577      0.406596     0.396944     0.222253     0.371371   \n","min       1.000000      0.000000     0.000000     0.000000     0.000000   \n","25%     480.750000      1.000000     0.000000     0.000000     0.000000   \n","50%     960.500000      1.000000     0.000000     0.000000     0.000000   \n","75%    1440.250000      1.000000     0.000000     0.000000     0.000000   \n","max    1920.000000      1.000000     1.000000     1.000000     1.000000   \n","\n","                DN          MYA         BRVO         TSLN          ERM  ...  \\\n","count  1920.000000  1920.000000  1920.000000  1920.000000  1920.000000  ...   \n","mean      0.071875     0.052604     0.038021     0.096875     0.007292  ...   \n","std       0.258348     0.223300     0.191296     0.295865     0.085102  ...   \n","min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n","25%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n","50%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n","75%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n","max       1.000000     1.000000     1.000000     1.000000     1.000000  ...   \n","\n","               CME         PTCR           CF           VH          MCA  \\\n","count  1920.000000  1920.000000  1920.000000  1920.000000  1920.000000   \n","mean      0.002083     0.002604     0.001563     0.000521     0.000521   \n","std       0.045608     0.050978     0.039508     0.022822     0.022822   \n","min       0.000000     0.000000     0.000000     0.000000     0.000000   \n","25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n","50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n","75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n","max       1.000000     1.000000     1.000000     1.000000     1.000000   \n","\n","                VS         BRAO          PLQ         HPED           CL  \n","count  1920.000000  1920.000000  1920.000000  1920.000000  1920.000000  \n","mean      0.000521     0.001042     0.000521     0.000521     0.000521  \n","std       0.022822     0.032266     0.022822     0.022822     0.022822  \n","min       0.000000     0.000000     0.000000     0.000000     0.000000  \n","25%       0.000000     0.000000     0.000000     0.000000     0.000000  \n","50%       0.000000     0.000000     0.000000     0.000000     0.000000  \n","75%       0.000000     0.000000     0.000000     0.000000     0.000000  \n","max       1.000000     1.000000     1.000000     1.000000     1.000000  \n","\n","[8 rows x 47 columns]"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["X_train.describe()\n","# No missing value"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"f6JFvyJLosaR"},"outputs":[],"source":["# reconstituion link image + drop ID feature\n","X_train['filename'] = X_train.apply(lambda x : \"data/Training_Set/Training/\" + str(x['ID']) + \".png\", axis=1)\n","X_val['filename'] = X_val.apply(lambda x : \"data/Evaluation_Set/Validation/\" + str(x['ID']) + \".png\", axis=1)\n","X_test['filename'] = X_test.apply(lambda x : \"data/Test_Set/Test/\" + str(x['ID']) + \".png\", axis=1)\n","\n","X_train = X_train.drop('ID', axis=1)\n","X_val = X_val.drop('ID', axis=1)\n","X_test = X_test.drop('ID', axis=1)"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"BmBvTNIFrwcu","outputId":"d26d7cf7-aba8-4f27-b0d8-758ed1a46352"},"outputs":[{"name":"stdout","output_type":"stream","text":["   Disease_Risk  DR  ARMD  MH  DN  MYA  BRVO  TSLN  ERM  LS  ...  PTCR  CF  \\\n","0             1   1     0   0   0    0     0     0    0   0  ...     0   0   \n","\n","   VH  MCA  VS  BRAO  PLQ  HPED  CL                          filename  \n","0   0    0   0     0    0     0   0  data/Training_Set/Training/1.png  \n","\n","[1 rows x 47 columns]\n","(1920, 47)\n"]}],"source":["print(X_train.head(1))\n","print(X_train.shape)\n","# 46 class + risk evaluation (47 features)"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"yyP20BwExLWT","outputId":"880a0f2c-bfdb-4a9e-eab4-24fc972a96c2"},"outputs":[{"name":"stdout","output_type":"stream","text":["shape of X_train: (1920,)\n","shape of Validation: (640,)\n","shape of y_train: (1920, 46)\n","shape of y_val: (640, 46)\n"]}],"source":["# datasets\n","X_train_img = X_train['filename']\n","X_val_img = X_val['filename']\n","y_train = X_train.drop(['filename'], axis=1)\n","y_val = X_val.drop(['filename'], axis=1)\n","\n","print('shape of X_train:', X_train_img.shape)\n","print('shape of Validation:', X_val_img.shape)\n","print('shape of y_train:', y_train.shape)\n","print('shape of y_val:', y_val.shape)"]},{"cell_type":"markdown","metadata":{"id":"Yt7zpW8M0woJ"},"source":["## Preprocessing images"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1aOunvgA0-B-"},"outputs":[],"source":["IMG_SHAPE = (300, 450)\n","BATCH_SIZE = 64"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nc90BUQD2A-I"},"outputs":[],"source":["@tf.function\n","\n","#Fonction pour prprocessing des images\n","def scale_down(img):\n","    img = tf.cast(img, dtype=tf.float32)\n","    img = tf.image.resize(img, (300, 450), method='nearest')\n","    img = (img / 255)\n","    \n","    return img\n","\n","#Preprocessing du jeu d'entrainement\n","def preprocessing_data(img):\n","   \n","    #Lecture et décodage des images:\n","    img = tf.io.read_file(img)\n","    img = tf.io.decode_png(img, channels=3)\n","\n","    #adjust contrast\n","    img =  tf.image.adjust_contrast(img, 1.35)\n","\n","    #Resize\n","    img = scale_down(img)\n","\n","    return img\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6tcDiKcXdlIJ","outputId":"c60fc1ac-aef6-457f-f263-d9a8b78868c5"},"outputs":[],"source":["#Datasets preprocessing\n","AUTO = tf.data.experimental.AUTOTUNE\n","\n","y_train = np.array(y_train).astype('float32')\n","y_val = np.array(y_val).astype('float32')\n","\n","dataset_train = tf.data.Dataset.from_tensor_slices((X_train_img, y_train))\n","dataset_val = tf.data.Dataset.from_tensor_slices((X_val_img, y_val))\n","\n","dataset_train=(dataset_train\n","               .shuffle(1000)\n","               .map(lambda x, y: [preprocessing_data(x), y], num_parallel_calls=AUTO)\n","               .batch(BATCH_SIZE, drop_remainder=True)\n","               .prefetch(AUTO)\n","               )\n","\n","dataset_val=(dataset_val\n","             .map(lambda x, y: [preprocessing_data(x), y], num_parallel_calls=AUTO)\n","             .batch(BATCH_SIZE, drop_remainder=True)\n","             .prefetch(AUTO)\n","             )\n","\n","\n","print(dataset_train)\n","print(dataset_val)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"59_JZb_I2qvI"},"outputs":[],"source":["def visualize(original, augmented):\n","    fig = plt.figure()\n","    plt.subplot(1,2,1)\n","    plt.title('Original image')\n","    plt.imshow(original)\n","    plt.axis('off')\n","\n","    plt.subplot(1,2,2)\n","    plt.title('Augmented image')\n","    plt.imshow(augmented)\n","    plt.axis('off')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RP1Is4ji2sZW","outputId":"a9ad09b5-5bd9-4a88-b9a2-a9f33c155a32"},"outputs":[],"source":["image, label = next(iter(dataset_train))\n","image, label = image.numpy()[0], label.numpy()[0]\n","\n","\n","flipped = tf.image.flip_left_right(image)\n","flipped =  tf.image.adjust_contrast(flipped, 1.35)\n","visualize(image, flipped)"]},{"cell_type":"markdown","metadata":{"id":"YudwlOWpUls7"},"source":["```\n","from keras.utils.data_utils import Sequence\n","from imblearn.over_sampling import RandomOverSampler\n","from imblearn.tensorflow import balanced_batch_generator\n","\n","\n","class BalancedDataGenerator(Sequence):\n","    \"\"\"ImageDataGenerator + RandomOversampling\"\"\"\n","    def __init__(self, x, y, datagen, batch_size=64):\n","        self.datagen = datagen\n","        self.batch_size = min(batch_size, x.shape[0])\n","        datagen.fit(x)\n","        self.gen, self.steps_per_epoch = balanced_batch_generator(x.reshape(x.shape[0], -1), y, sampler=RandomOverSampler(), batch_size=self.batch_size, keep_sparse=True)\n","        self._shape = (self.steps_per_epoch * batch_size, *x.shape[1:])\n","        \n","    def __len__(self):\n","        return self.steps_per_epoch\n","\n","\n","    def __getitem__(self, idx):\n","        x_batch, y_batch = self.gen.__next__()\n","        x_batch = x_batch.reshape(-1, *self._shape[1:])\n","        return self.datagen.flow(x_batch, y_batch, batch_size=self.batch_size).next()\n","\n","balanced_gen = BalancedDataGenerator(X_train_path, y_train, train_generator, batch_size=64)\n","#balanced_gen_val = BalancedDataGenerator(X_val, y_val, train_generator, batch_size=64)\n","steps_per_epoch = balanced_gen.steps_per_epoch\n","\n","```\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-tK-31cE3YMT","outputId":"07352639-47b7-44a3-e1da-584b2a240ea3"},"outputs":[],"source":["#API keras preparation\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Conv2D, Dropout, BatchNormalization, Activation, MaxPool2D, Dense, Flatten, GlobalAvgPool2D\n","from keras import backend as K\n","from tensorflow.keras.applications import VGG16\n","vgg16 = VGG16()\n","\n","#for layer in xception.layers:\n","#    print(layer.name, layer)"]},{"cell_type":"markdown","metadata":{"id":"yvKiDs5s7Wzv"},"source":["## Global architecture VGG16:\n","________________________________________________________________________________\n","<img src=\"https://datascientest.com/wp-content/uploads/2021/04/illu_VGG-02.png\" alt=\"data2\" align=\"top\" style=\"width: 800px;\">\n"]},{"cell_type":"markdown","metadata":{"id":"5ZbKSepW7kHc"},"source":["## Classification model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y9EDITPj877b","outputId":"9d395dd5-c528-4b09-ef3b-83c54e28c20a"},"outputs":[],"source":["shape = (250, 400,3)\n","\n","def Layers(inputs, trainable=False):\n","    global vgg16_model\n","    vgg16_model = VGG16(weights='imagenet',\n","                        include_top=False,\n","                        input_tensor=inputs)\n","    \n","    if trainable == True:\n","        for layer in vgg16_model.layers:\n","            layer.trainable = True\n","            \n","    else:\n","        vgg16_model.trainable = False\n","            \n","    return vgg16_model.output\n","    \n","        \n","def Build_VGG16(trainable=False):\n","    \n","    inputs = Input(shape=shape)\n","    vgg16 = Layers(inputs, trainable)\n","\n","    conv1 = Flatten()(vgg16_model.output)\n","    \n","    dense2 = Dense(256,activation='relu')(conv1)\n","    dense2 = Dropout(rate=0.2)(dense2)\n","    \n","    dense3 = Dense(128,activation='relu')(dense2)\n","    dense3 = Dropout(rate=0.2)(dense3)\n","    \n","    model = Dense(46,activation= 'sigmoid')(dense3)\n","    \n","    return Model(inputs=inputs, outputs = model)\n","\n","model = Build_VGG16()\n","model.summary()"]},{"cell_type":"markdown","metadata":{"id":"utde5o_BF5py"},"source":["\n","\n","*   use a LR function to adapt Gradient\n","*   Class imbalanced, we create a loss fonction to adjust weight \n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MtAyPNC0YlxM"},"outputs":[],"source":["class LossLearningRateScheduler(tf.keras.callbacks.History):\n","    \"\"\"\n","    base_lr: the starting learning rate\n","    lookback_epochs: the number of epochs in the past to compare with the loss function at the current epoch to determine if progress is being made.\n","    decay_threshold / decay_multiple: if loss function has not improved by a factor of decay_threshold * lookback_epochs, then decay_multiple will be applied to the learning rate.\n","    spike_epochs: list of the epoch numbers where you want to spike the learning rate.\n","    spike_multiple: the multiple applied to the current learning rate for a spike.\n","    \"\"\"\n","\n","    def __init__(self, base_lr, lookback_epochs, spike_epochs = None, spike_multiple = 10, decay_threshold = 0.002, decay_multiple = 0.7, loss_type = 'val_loss'):\n","\n","        super(LossLearningRateScheduler, self).__init__()\n","        self.base_lr = base_lr\n","        self.lookback_epochs = lookback_epochs\n","        self.spike_epochs = spike_epochs\n","        self.spike_multiple = spike_multiple\n","        self.decay_threshold = decay_threshold\n","        self.decay_multiple = decay_multiple\n","        self.loss_type = loss_type\n","\n","\n","    def on_epoch_begin(self, epoch, logs=None):\n","\n","        if len(self.epoch) > self.lookback_epochs:\n","            current_lr = tf.keras.backend.get_value(self.model.optimizer.lr)\n","            target_loss = self.history[self.loss_type] \n","            loss_diff =  target_loss[-int(self.lookback_epochs)] - target_loss[-1]\n","\n","\n","            if loss_diff <= np.abs(target_loss[-1]) * (self.decay_threshold * self.lookback_epochs):\n","                print(' '.join(('Changing learning rate from', str(current_lr), 'to', str(current_lr * self.decay_multiple))))\n","                tf.keras.backend.set_value(self.model.optimizer.lr, current_lr * self.decay_multiple)\n","                current_lr = current_lr * self.decay_multiple\n","\n","            else:\n","                print(' '.join(('Learning rate:', str(current_lr))))\n","\n","            if self.spike_epochs is not None and len(self.epoch) in self.spike_epochs:\n","                print(' '.join(('Spiking learning rate from', str(current_lr), 'to', str(current_lr * self.spike_multiple))))\n","                tf.keras.backend.set_value(self.model.optimizer.lr, current_lr * self.spike_multiple)\n","\n","        else:\n","            print(' '.join(('Setting learning rate to', str(self.base_lr))))\n","            tf.keras.backend.set_value(self.model.optimizer.lr, self.base_lr)\n","\n","        return tf.keras.backend.get_value(self.model.optimizer.lr)\n","\n","callback_lr = LossLearningRateScheduler(base_lr=0.001, lookback_epochs=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"97hEQDUOhNk5"},"outputs":[],"source":["#Re-Weighting classes binary crossentropy\n","\n","def dyn_weighted_bincrossentropy(true, pred):\n","\n","    # get the total number of inputs\n","    num_pred = K.sum(K.cast(pred < 0.5, true.dtype)) + K.sum(true)\n","    # get weight of values in 'pos' category\n","    zero_weight =  K.sum(true)/ num_pred +  K.epsilon() \n","    # get weight of values in 'false' category\n","    one_weight = K.sum(K.cast(pred < 0.5, true.dtype)) / num_pred +  K.epsilon()\n","    # calculate the weight vector\n","    weights =  (1.0 - true) * zero_weight +  true * one_weight \n","    # calculate the binary cross entropy\n","    bin_crossentropy = K.binary_crossentropy(true, pred)\n","    # apply the weights\n","    weighted_bin_crossentropy = weights * bin_crossentropy \n","\n","    return K.mean(weighted_bin_crossentropy)\n","\n","\n","def weighted_bincrossentropy(true, pred, weight_zero = 0.25, weight_one = 1):\n","\n","    # calculate the binary cross entropy\n","    bin_crossentropy = K.binary_crossentropy(true, pred)\n","    # apply the weights\n","    weights = true * weight_one + (1. - true) * weight_zero\n","    weighted_bin_crossentropy = weights * bin_crossentropy \n","\n","    return K.mean(weighted_bin_crossentropy)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ycYqRxWMG2JZ"},"outputs":[],"source":["from tensorflow.keras.metrics import AUC\n","pr_metric = AUC(curve='PR', num_thresholds=5000, from_logits=True, name='pr_metric') # The higher the threshold value, the more accurate it is calculated.\n","roc_metric = AUC(curve='ROC', num_thresholds=5000, from_logits=True, name='roc_metric') \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YH4YEjiuAL8Z"},"outputs":[],"source":["model.compile(loss=dyn_weighted_bincrossentropy,\n","              optimizer =tf.keras.optimizers.Adam(),\n","              metrics= [roc_metric, pr_metric])\n","\n","#weighted_binary_crossentropy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q3AaM8SbAium","outputId":"c45beb16-6764-4b5a-e5aa-fe190a55599d"},"outputs":[],"source":["tf.keras.utils.plot_model(model, 'retinal_output_model.png', show_shapes=True, dpi=100)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vJ2VfM1cAp_Z","outputId":"5b16cf6b-eacf-45be-9fc3-f3f8fd181690"},"outputs":[],"source":["history = model.fit(dataset_train,\n","                    validation_data=dataset_val,\n","                    epochs=15, \n","                    verbose=1, \n","                   callbacks=callback_lr)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UuKKLGhBf19D","outputId":"103a5cf9-3ff2-4eed-fdc8-8ac0357f9089"},"outputs":[],"source":["from keras.models import model_from_json\n","\n","model_archtecture = model.to_json()\n","\n","with open('retinal_model.json', 'w') as json_file:\n","    json_file.write(model_archtecture)\n","\n","model.save('./retinal_model')\n","model.save_weights('./retinal_model.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q8cjA6SyhGxn","outputId":"80f64acd-7ef7-44ba-b882-cc900765201b"},"outputs":[],"source":[" !zip -r /content/retinal_model.zip /content/retinal_model"]},{"cell_type":"markdown","metadata":{"id":"rYBSNaewOTPc"},"source":["## Predictions and ROC/PR curves on X_test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KHFHG-VpOYrB","outputId":"d7be2d06-0bc6-4ef0-f4cb-9c3b5c8bcb5f"},"outputs":[],"source":["#preparation and preprocesing\n","X_test_path = X_test['filename']\n","y_test = X_test.drop(['filename'], axis=1)\n","y_test = np.array(y_test).astype('float32')\n","\n","print('shape of X_test:', X_test_img.shape)\n","print('shape of y_test:', y_test.shape)\n","\n","X_test_img  = []\n","for filepath in tqdm(X_test_path):\n","\n","  #Read and decode\n","  img = tf.io.read_file(filepath)\n","  img = tf.io.decode_png(img, channels=3)\n","\n","  #adjust contrast\n","  img =  tf.image.adjust_contrast(img, 1.5)\n","\n","  #Resize\n","  img = scale_down(img)\n","  X_test_img.append([img])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5z8Bpd42XUn5","outputId":"84939e7a-fd81-41e1-c810-a08dd4a78523"},"outputs":[],"source":["#transform to array numpy\n","X_test_img = np.array(X_test_img)\n","X_test_img = X_test_img[:,0,:,:]\n","X_test_img.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kjZH5agGYFWo"},"outputs":[],"source":["y_pred = model.predict(X_test_img)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PA_rT9GJc4y0","outputId":"a254b014-e8bd-4437-ee71-7b0a271b077c"},"outputs":[],"source":["from sklearn.metrics import roc_auc_score\n","\n","auc_scores = []\n","for i in range(46):\n","  try:\n","    auc = roc_auc_score(y_test[:,i], y_pred[:,i])\n","    auc_scores.append(auc)\n","  except:\n","    pass\n","\n","\n","def Average(lst):\n","    return sum(lst) / len(lst)\n","  \n","avg_auc = Average(auc_scores)\n","  \n","# Printing average of the list\n","print(\"Average auc score available classes =\", round(avg_auc, 2),'%')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XRs6TgmypdXd"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":4}
