{"cells":[{"cell_type":"markdown","metadata":{},"source":["**Import Tensorflow & Varify GPU**\n","- [MacOS](https://medium.com/geekculture/installing-tensorflow-on-apple-silicon-84a28050d784)"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Found GPU at: /device:GPU:0\n"]},{"name":"stderr","output_type":"stream","text":["2023-12-27 22:58:47.100798: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n","2023-12-27 22:58:47.100818: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n","2023-12-27 22:58:47.100823: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n","2023-12-27 22:58:47.100872: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n","2023-12-27 22:58:47.100908: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"]}],"source":["import tensorflow as tf\n","\n","device_name = tf.test.gpu_device_name()\n","if not device_name:\n","    print(\"GPU device not found\")\n","else:\n","    print(f\"Found GPU at: {device_name}\")"]},{"cell_type":"markdown","metadata":{"id":"Pp_x_psEty8M"},"source":["**Packages and data preprocessing**"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"p-BOet9PrCgd","outputId":"258c7072-5f27-476e-f7b8-2bc71cdf88c7"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'seaborn'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"]}],"source":["import os \n","from tqdm import tqdm\n","from glob import glob\n","import cv2\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np\n","import tensorflow as tf\n","\n","from warnings import filterwarnings\n","filterwarnings('ignore')\n","\n","\n","print(os.getcwd())\n","path_cwd = os.getcwd()\n","\n","X_train = pd.read_csv('/data/Training_Labels.csv')\n","X_val = pd.read_csv('/data/Validation_Labels.csv')\n","X_test = pd.read_csv('/data/Test_Labels.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KOWOE7D7pGJY","outputId":"9e88622d-5bba-4a98-c349-65d713e3571e"},"outputs":[],"source":["X_train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pu9zLg54qHPH","outputId":"4369b563-439f-4f88-ca4e-907c7e90324d"},"outputs":[],"source":["X_train.describe()\n","#No missing value"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f6JFvyJLosaR"},"outputs":[],"source":["#reconstituion link image + drop ID feature\n","X_train['filename'] = X_train.apply(lambda x : \"/content/Training/\" +str(x['ID']) + \".png\", axis=1)\n","X_val['filename'] = X_val.apply(lambda x : \"/content/Validation/\" +str(x['ID']) + \".png\", axis=1)\n","X_test['filename'] = X_test.apply(lambda x : \"/content/Test/\" +str(x['ID']) + \".png\", axis=1)\n","\n","X_train = X_train.drop('ID', axis=1)\n","X_val = X_val.drop('ID', axis=1)\n","X_test = X_test.drop('ID', axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BmBvTNIFrwcu","outputId":"d26d7cf7-aba8-4f27-b0d8-758ed1a46352"},"outputs":[],"source":["print(X_train.head(1))\n","print(X_train.shape)\n","#46 class + risk evaluation (47 features)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yyP20BwExLWT","outputId":"880a0f2c-bfdb-4a9e-eab4-24fc972a96c2"},"outputs":[],"source":["#datasets\n","X_train_img = X_train['filename']\n","X_val_img = X_val['filename']\n","y_train = X_train.drop(['filename'], axis=1)\n","y_val = X_val.drop(['filename'], axis=1)\n","\n","print('shape of X_train:', X_train_img.shape)\n","print('shape of Validation:', X_val_img.shape)\n","print('shape of y_train:', y_train.shape)\n","print('shape of y_val:', y_val.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RYBSN3Oi9lxG"},"outputs":[],"source":["#Plot some random images\n","import cv2\n","import random\n","def plotImages():\n","    i=1\n","    plt.figure(figsize=(15,10))\n","    for r in random.sample(glob(path_cwd + '/Training/**'), 15):\n","      plt.subplot(3,5,i)\n","      img = cv2.imread(r)\n","      img = tf.reverse(img, axis=[-1])\n","      img =  tf.image.adjust_contrast(img, 1.5)\n","      plt.imshow(img)\n","      i+=1\n","      plt.axis('off')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dromFwjw96lM","outputId":"f1f68009-b8da-4d63-9017-9fa097bec119"},"outputs":[],"source":["plotImages() #Seems we have to convert to RGB format"]},{"cell_type":"markdown","metadata":{"id":"Yt7zpW8M0woJ"},"source":["## Preprocessing images"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1aOunvgA0-B-"},"outputs":[],"source":["IMG_SHAPE = (300, 450)\n","BATCH_SIZE = 64"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nc90BUQD2A-I"},"outputs":[],"source":["@tf.function\n","\n","#Fonction pour prprocessing des images\n","def scale_down(img):\n","    img = tf.cast(img, dtype=tf.float32)\n","    img = tf.image.resize(img, (300, 450), method='nearest')\n","    img = (img / 255)\n","    \n","    return img\n","\n","#Preprocessing du jeu d'entrainement\n","def preprocessing_data(img):\n","   \n","    #Lecture et d√©codage des images:\n","    img = tf.io.read_file(img)\n","    img = tf.io.decode_png(img, channels=3)\n","\n","    #adjust contrast\n","    img =  tf.image.adjust_contrast(img, 1.35)\n","\n","    #Resize\n","    img = scale_down(img)\n","\n","    return img\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6tcDiKcXdlIJ","outputId":"c60fc1ac-aef6-457f-f263-d9a8b78868c5"},"outputs":[],"source":["#Datasets preprocessing\n","AUTO = tf.data.experimental.AUTOTUNE\n","\n","y_train = np.array(y_train).astype('float32')\n","y_val = np.array(y_val).astype('float32')\n","\n","dataset_train = tf.data.Dataset.from_tensor_slices((X_train_img, y_train))\n","dataset_val = tf.data.Dataset.from_tensor_slices((X_val_img, y_val))\n","\n","dataset_train=(dataset_train\n","               .shuffle(1000)\n","               .map(lambda x, y: [preprocessing_data(x), y], num_parallel_calls=AUTO)\n","               .batch(BATCH_SIZE, drop_remainder=True)\n","               .prefetch(AUTO)\n","               )\n","\n","dataset_val=(dataset_val\n","             .map(lambda x, y: [preprocessing_data(x), y], num_parallel_calls=AUTO)\n","             .batch(BATCH_SIZE, drop_remainder=True)\n","             .prefetch(AUTO)\n","             )\n","\n","\n","print(dataset_train)\n","print(dataset_val)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"59_JZb_I2qvI"},"outputs":[],"source":["def visualize(original, augmented):\n","    fig = plt.figure()\n","    plt.subplot(1,2,1)\n","    plt.title('Original image')\n","    plt.imshow(original)\n","    plt.axis('off')\n","\n","    plt.subplot(1,2,2)\n","    plt.title('Augmented image')\n","    plt.imshow(augmented)\n","    plt.axis('off')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RP1Is4ji2sZW","outputId":"a9ad09b5-5bd9-4a88-b9a2-a9f33c155a32"},"outputs":[],"source":["image, label = next(iter(dataset_train))\n","image, label = image.numpy()[0], label.numpy()[0]\n","\n","\n","flipped = tf.image.flip_left_right(image)\n","flipped =  tf.image.adjust_contrast(flipped, 1.35)\n","visualize(image, flipped)"]},{"cell_type":"markdown","metadata":{"id":"YudwlOWpUls7"},"source":["```\n","from keras.utils.data_utils import Sequence\n","from imblearn.over_sampling import RandomOverSampler\n","from imblearn.tensorflow import balanced_batch_generator\n","\n","\n","class BalancedDataGenerator(Sequence):\n","    \"\"\"ImageDataGenerator + RandomOversampling\"\"\"\n","    def __init__(self, x, y, datagen, batch_size=64):\n","        self.datagen = datagen\n","        self.batch_size = min(batch_size, x.shape[0])\n","        datagen.fit(x)\n","        self.gen, self.steps_per_epoch = balanced_batch_generator(x.reshape(x.shape[0], -1), y, sampler=RandomOverSampler(), batch_size=self.batch_size, keep_sparse=True)\n","        self._shape = (self.steps_per_epoch * batch_size, *x.shape[1:])\n","        \n","    def __len__(self):\n","        return self.steps_per_epoch\n","\n","\n","    def __getitem__(self, idx):\n","        x_batch, y_batch = self.gen.__next__()\n","        x_batch = x_batch.reshape(-1, *self._shape[1:])\n","        return self.datagen.flow(x_batch, y_batch, batch_size=self.batch_size).next()\n","\n","balanced_gen = BalancedDataGenerator(X_train_path, y_train, train_generator, batch_size=64)\n","#balanced_gen_val = BalancedDataGenerator(X_val, y_val, train_generator, batch_size=64)\n","steps_per_epoch = balanced_gen.steps_per_epoch\n","\n","```\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-tK-31cE3YMT","outputId":"07352639-47b7-44a3-e1da-584b2a240ea3"},"outputs":[],"source":["#API keras preparation\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Conv2D, Dropout, BatchNormalization, Activation, MaxPool2D, Dense, Flatten, GlobalAvgPool2D\n","from keras import backend as K\n","from tensorflow.keras.applications import VGG16\n","vgg16 = VGG16()\n","\n","#for layer in xception.layers:\n","#    print(layer.name, layer)"]},{"cell_type":"markdown","metadata":{"id":"yvKiDs5s7Wzv"},"source":["## Global architecture VGG16:\n","________________________________________________________________________________\n","<img src=\"https://datascientest.com/wp-content/uploads/2021/04/illu_VGG-02.png\" alt=\"data2\" align=\"top\" style=\"width: 800px;\">\n"]},{"cell_type":"markdown","metadata":{"id":"5ZbKSepW7kHc"},"source":["## Classification model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y9EDITPj877b","outputId":"9d395dd5-c528-4b09-ef3b-83c54e28c20a"},"outputs":[],"source":["shape = (250, 400,3)\n","\n","def Layers(inputs, trainable=False):\n","    global vgg16_model\n","    vgg16_model = VGG16(weights='imagenet',\n","                        include_top=False,\n","                        input_tensor=inputs)\n","    \n","    if trainable == True:\n","        for layer in vgg16_model.layers:\n","            layer.trainable = True\n","            \n","    else:\n","        vgg16_model.trainable = False\n","            \n","    return vgg16_model.output\n","    \n","        \n","def Build_VGG16(trainable=False):\n","    \n","    inputs = Input(shape=shape)\n","    vgg16 = Layers(inputs, trainable)\n","\n","    conv1 = Flatten()(vgg16_model.output)\n","    \n","    dense2 = Dense(256,activation='relu')(conv1)\n","    dense2 = Dropout(rate=0.2)(dense2)\n","    \n","    dense3 = Dense(128,activation='relu')(dense2)\n","    dense3 = Dropout(rate=0.2)(dense3)\n","    \n","    model = Dense(46,activation= 'sigmoid')(dense3)\n","    \n","    return Model(inputs=inputs, outputs = model)\n","\n","model = Build_VGG16()\n","model.summary()"]},{"cell_type":"markdown","metadata":{"id":"utde5o_BF5py"},"source":["\n","\n","*   use a LR function to adapt Gradient\n","*   Class imbalanced, we create a loss fonction to adjust weight \n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MtAyPNC0YlxM"},"outputs":[],"source":["class LossLearningRateScheduler(tf.keras.callbacks.History):\n","    \"\"\"\n","    base_lr: the starting learning rate\n","    lookback_epochs: the number of epochs in the past to compare with the loss function at the current epoch to determine if progress is being made.\n","    decay_threshold / decay_multiple: if loss function has not improved by a factor of decay_threshold * lookback_epochs, then decay_multiple will be applied to the learning rate.\n","    spike_epochs: list of the epoch numbers where you want to spike the learning rate.\n","    spike_multiple: the multiple applied to the current learning rate for a spike.\n","    \"\"\"\n","\n","    def __init__(self, base_lr, lookback_epochs, spike_epochs = None, spike_multiple = 10, decay_threshold = 0.002, decay_multiple = 0.7, loss_type = 'val_loss'):\n","\n","        super(LossLearningRateScheduler, self).__init__()\n","        self.base_lr = base_lr\n","        self.lookback_epochs = lookback_epochs\n","        self.spike_epochs = spike_epochs\n","        self.spike_multiple = spike_multiple\n","        self.decay_threshold = decay_threshold\n","        self.decay_multiple = decay_multiple\n","        self.loss_type = loss_type\n","\n","\n","    def on_epoch_begin(self, epoch, logs=None):\n","\n","        if len(self.epoch) > self.lookback_epochs:\n","            current_lr = tf.keras.backend.get_value(self.model.optimizer.lr)\n","            target_loss = self.history[self.loss_type] \n","            loss_diff =  target_loss[-int(self.lookback_epochs)] - target_loss[-1]\n","\n","\n","            if loss_diff <= np.abs(target_loss[-1]) * (self.decay_threshold * self.lookback_epochs):\n","                print(' '.join(('Changing learning rate from', str(current_lr), 'to', str(current_lr * self.decay_multiple))))\n","                tf.keras.backend.set_value(self.model.optimizer.lr, current_lr * self.decay_multiple)\n","                current_lr = current_lr * self.decay_multiple\n","\n","            else:\n","                print(' '.join(('Learning rate:', str(current_lr))))\n","\n","            if self.spike_epochs is not None and len(self.epoch) in self.spike_epochs:\n","                print(' '.join(('Spiking learning rate from', str(current_lr), 'to', str(current_lr * self.spike_multiple))))\n","                tf.keras.backend.set_value(self.model.optimizer.lr, current_lr * self.spike_multiple)\n","\n","        else:\n","            print(' '.join(('Setting learning rate to', str(self.base_lr))))\n","            tf.keras.backend.set_value(self.model.optimizer.lr, self.base_lr)\n","\n","        return tf.keras.backend.get_value(self.model.optimizer.lr)\n","\n","callback_lr = LossLearningRateScheduler(base_lr=0.001, lookback_epochs=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"97hEQDUOhNk5"},"outputs":[],"source":["#Re-Weighting classes binary crossentropy\n","\n","def dyn_weighted_bincrossentropy(true, pred):\n","\n","    # get the total number of inputs\n","    num_pred = K.sum(K.cast(pred < 0.5, true.dtype)) + K.sum(true)\n","    # get weight of values in 'pos' category\n","    zero_weight =  K.sum(true)/ num_pred +  K.epsilon() \n","    # get weight of values in 'false' category\n","    one_weight = K.sum(K.cast(pred < 0.5, true.dtype)) / num_pred +  K.epsilon()\n","    # calculate the weight vector\n","    weights =  (1.0 - true) * zero_weight +  true * one_weight \n","    # calculate the binary cross entropy\n","    bin_crossentropy = K.binary_crossentropy(true, pred)\n","    # apply the weights\n","    weighted_bin_crossentropy = weights * bin_crossentropy \n","\n","    return K.mean(weighted_bin_crossentropy)\n","\n","\n","def weighted_bincrossentropy(true, pred, weight_zero = 0.25, weight_one = 1):\n","\n","    # calculate the binary cross entropy\n","    bin_crossentropy = K.binary_crossentropy(true, pred)\n","    # apply the weights\n","    weights = true * weight_one + (1. - true) * weight_zero\n","    weighted_bin_crossentropy = weights * bin_crossentropy \n","\n","    return K.mean(weighted_bin_crossentropy)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ycYqRxWMG2JZ"},"outputs":[],"source":["from tensorflow.keras.metrics import AUC\n","pr_metric = AUC(curve='PR', num_thresholds=5000, from_logits=True, name='pr_metric') # The higher the threshold value, the more accurate it is calculated.\n","roc_metric = AUC(curve='ROC', num_thresholds=5000, from_logits=True, name='roc_metric') \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YH4YEjiuAL8Z"},"outputs":[],"source":["model.compile(loss=dyn_weighted_bincrossentropy,\n","              optimizer =tf.keras.optimizers.Adam(),\n","              metrics= [roc_metric, pr_metric])\n","\n","#weighted_binary_crossentropy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q3AaM8SbAium","outputId":"c45beb16-6764-4b5a-e5aa-fe190a55599d"},"outputs":[],"source":["tf.keras.utils.plot_model(model, 'retinal_output_model.png', show_shapes=True, dpi=100)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vJ2VfM1cAp_Z","outputId":"5b16cf6b-eacf-45be-9fc3-f3f8fd181690"},"outputs":[],"source":["history = model.fit(dataset_train,\n","                    validation_data=dataset_val,\n","                    epochs=15, \n","                    verbose=1, \n","                   callbacks=callback_lr)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UuKKLGhBf19D","outputId":"103a5cf9-3ff2-4eed-fdc8-8ac0357f9089"},"outputs":[],"source":["from keras.models import model_from_json\n","\n","model_archtecture = model.to_json()\n","\n","with open('retinal_model.json', 'w') as json_file:\n","    json_file.write(model_archtecture)\n","\n","model.save('./retinal_model')\n","model.save_weights('./retinal_model.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q8cjA6SyhGxn","outputId":"80f64acd-7ef7-44ba-b882-cc900765201b"},"outputs":[],"source":[" !zip -r /content/retinal_model.zip /content/retinal_model"]},{"cell_type":"markdown","metadata":{"id":"rYBSNaewOTPc"},"source":["## Predictions and ROC/PR curves on X_test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KHFHG-VpOYrB","outputId":"d7be2d06-0bc6-4ef0-f4cb-9c3b5c8bcb5f"},"outputs":[],"source":["#preparation and preprocesing\n","X_test_path = X_test['filename']\n","y_test = X_test.drop(['filename'], axis=1)\n","y_test = np.array(y_test).astype('float32')\n","\n","print('shape of X_test:', X_test_img.shape)\n","print('shape of y_test:', y_test.shape)\n","\n","X_test_img  = []\n","for filepath in tqdm(X_test_path):\n","\n","  #Read and decode\n","  img = tf.io.read_file(filepath)\n","  img = tf.io.decode_png(img, channels=3)\n","\n","  #adjust contrast\n","  img =  tf.image.adjust_contrast(img, 1.5)\n","\n","  #Resize\n","  img = scale_down(img)\n","  X_test_img.append([img])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5z8Bpd42XUn5","outputId":"84939e7a-fd81-41e1-c810-a08dd4a78523"},"outputs":[],"source":["#transform to array numpy\n","X_test_img = np.array(X_test_img)\n","X_test_img = X_test_img[:,0,:,:]\n","X_test_img.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kjZH5agGYFWo"},"outputs":[],"source":["y_pred = model.predict(X_test_img)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PA_rT9GJc4y0","outputId":"a254b014-e8bd-4437-ee71-7b0a271b077c"},"outputs":[],"source":["from sklearn.metrics import roc_auc_score\n","\n","auc_scores = []\n","for i in range(46):\n","  try:\n","    auc = roc_auc_score(y_test[:,i], y_pred[:,i])\n","    auc_scores.append(auc)\n","  except:\n","    pass\n","\n","\n","def Average(lst):\n","    return sum(lst) / len(lst)\n","  \n","avg_auc = Average(auc_scores)\n","  \n","# Printing average of the list\n","print(\"Average auc score available classes =\", round(avg_auc, 2),'%')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XRs6TgmypdXd"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":4}
